{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import boto3\n",
    "from io import BytesIO\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from scrapy import Selector\n",
    "import math\n",
    "import requests\n",
    "import json\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score, classification_report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client('s3',\n",
    "                  aws_access_key_id='ASIAYAAO5HRMHQA4FOKO',\n",
    "                  aws_secret_access_key='q3Q4GVCN/bH235x8bi9xjWwYauW9yDB3zFuJahDN',\n",
    "                  aws_session_token='IQoJb3JpZ2luX2VjEL3//////////wEaCXVzLWVhc3QtMiJIMEYCIQDTs95dhyT+0P8nRoaaKzPBG6EUIU7sHo2CDFJE56XwBQIhAPZYM89S9QpKDjiYImeV1/glUWJARbLOJ8UVNlIHPTSDKusCCCYQABoMNTQ5Nzg3MDkwMDA4Igw9rYt3Mfm/corPeRcqyAKL3uK+Gtpkhq5czRYC1QLzigjBpGg4d16iRv/nnd92ccmfOoB1jMkwiJZevR0jQlUThLpMLX5w/dj9GO2beyBA3RMlrL95/nBTJBm+BAhGM/KUqX7NjLsxbqvEAZtGFCemC8fmAU/p8DBsOGtr9yeVvTCd69YjeCidsT2+z2eqc8aCOCh1zj19hHEFlgGwKRvBMPoqZL/yJOwNCteGWwyUYZj7X/d1llzCp0GLKODuYXxiXh1CGq7kT8t/rVHMksBc3gW35gziV2rDYAXSfEAGtG+ynVU6EDRM+KdfOE6+AwGBL5JTEVLg0WJtnxmq4u6JfmwD5/Dlor9QY7iInAOjbpdT+8jglEjjpZI76j+Rd3AZdzfP+aMts8JgvRaohdQbRCK1cfND4kAGjj5QajKyFFoGr8L0IbJ1QTPbyPINrSWAkVyF1ybBMLHb9rEGOqYBAruEuKhg/lEWdXfBtxvr2yEK41RaQbtrdtAXrviXadOrjzE/O31LWHzHGUEzbo5qIvX+BJ+SEv0Xz8lMK6m06ZOsKYxhAj1bd3XMMJ3nDbdaBBuMrbVOBCPGOfPmeSGMs+w2F/EFt+pYwa7JjLM5FQ6lBWmztH5MVktYyJTlUVrlc6zdrr3OOAU9AUtZdD9Bfo6aLLFI/oRtUKU2JKDyf5MvYXfDjg==')\n",
    "\n",
    "\n",
    "bucket_name = 'de300spring2024'\n",
    "object_key = 'Wilson_Ting/heart_disease.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_obj = s3.get_object(Bucket=bucket_name, Key=object_key)\n",
    "body = csv_obj['Body']\n",
    "csv_string = body.read().decode('utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     age  sex  painloc  painexer  cp  trestbps  smoke  fbs  prop  nitr  pro  \\\n",
      "0     63    1      1.0       1.0   1     145.0    NaN  1.0   0.0   0.0  0.0   \n",
      "1     67    1      1.0       1.0   4     160.0    NaN  0.0   1.0   0.0  0.0   \n",
      "2     67    1      1.0       1.0   4     120.0    NaN  0.0   1.0   0.0  0.0   \n",
      "3     37    1      1.0       1.0   3     130.0    NaN  0.0   1.0   0.0  0.0   \n",
      "4     41    0      1.0       1.0   2     130.0    NaN  0.0   0.0   0.0  0.0   \n",
      "..   ...  ...      ...       ...  ..       ...    ...  ...   ...   ...  ...   \n",
      "894   54    1      1.0       1.0   4     180.0    NaN  0.0   0.0   0.0  0.0   \n",
      "895   56    1      1.0       1.0   4     125.0    1.0  1.0   0.0   0.0  0.0   \n",
      "896   56    1      0.0       1.0   3     125.0    NaN  0.0   0.0   0.0  0.0   \n",
      "897   54    1      1.0       1.0   4     130.0    NaN  0.0   0.0   0.0  0.0   \n",
      "898   66    0      1.0       1.0   4     155.0    NaN  0.0   0.0   0.0  0.0   \n",
      "\n",
      "     diuretic  thaldur  thalach  exang   oldpeak  slope  target  source_1  \\\n",
      "0         0.0     10.5    150.0    0.0  2.300000    3.0       0       NaN   \n",
      "1         0.0      9.5    108.0    1.0  1.500000    2.0       1       NaN   \n",
      "2         0.0      8.5    129.0    1.0  2.600000    2.0       1       NaN   \n",
      "3         0.0     13.0    187.0    0.0  3.500000    3.0       0       NaN   \n",
      "4         0.0      7.0    172.0    0.0  1.400000    1.0       0       NaN   \n",
      "..        ...      ...      ...    ...       ...    ...     ...       ...   \n",
      "894       0.0      7.5    150.0    0.0  1.500000    2.0       1       NaN   \n",
      "895       0.0      6.5    103.0    1.0  1.000000    2.0       1       1.0   \n",
      "896       0.0      6.0     98.0    0.0  0.873293    2.0       1       NaN   \n",
      "897       0.0      9.5    110.0    1.0  3.000000    2.0       1       NaN   \n",
      "898       0.0      6.5     90.0    0.0  0.000000    2.0       1       NaN   \n",
      "\n",
      "     source_2  \n",
      "0         NaN  \n",
      "1         NaN  \n",
      "2         NaN  \n",
      "3         NaN  \n",
      "4         NaN  \n",
      "..        ...  \n",
      "894       NaN  \n",
      "895       1.0  \n",
      "896       NaN  \n",
      "897       NaN  \n",
      "898       NaN  \n",
      "\n",
      "[899 rows x 20 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23/2356326126.py:12: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['painloc'].fillna(painloc_mode, inplace=True)\n",
      "/tmp/ipykernel_23/2356326126.py:15: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['painexer'].fillna(painexer_mode, inplace=True)\n",
      "/tmp/ipykernel_23/2356326126.py:29: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['thaldur'].fillna(thaldur_mean, inplace=True)\n",
      "/tmp/ipykernel_23/2356326126.py:30: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['thalach'].fillna(thalach_mean, inplace=True)\n",
      "/tmp/ipykernel_23/2356326126.py:32: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df[columns_to_replace] = df[columns_to_replace].applymap(\n",
      "/tmp/ipykernel_23/2356326126.py:37: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[column].fillna(mode_value, inplace=True)\n",
      "/tmp/ipykernel_23/2356326126.py:40: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['exang'].fillna(exang_mode, inplace=True)\n",
      "/tmp/ipykernel_23/2356326126.py:41: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['slope'].fillna(slope_mode, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(BytesIO(csv_string.encode()))\n",
    "retain = [\n",
    "    'age', 'sex', 'painloc', 'painexer', 'cp', 'trestbps', 'smoke',\n",
    "    'fbs', 'prop', 'nitr', 'pro', 'diuretic', 'thaldur', 'thalach', 'exang',\n",
    "    'oldpeak', 'slope'\n",
    "]\n",
    "selected_columns_with_target = retain + ['target']\n",
    "df = df.loc[:, selected_columns_with_target]\n",
    "painloc_mode = df['painloc'].dropna().mode()[0]\n",
    "painexer_mode = df['painexer'].dropna().mode()[0]\n",
    "# Replace NaN in 'painloc' with its mode\n",
    "df['painloc'].fillna(painloc_mode, inplace=True)\n",
    "\n",
    "# Replace NaN in 'painexer' with its mode\n",
    "df['painexer'].fillna(painexer_mode, inplace=True)\n",
    "\n",
    "values_above_100 = df['trestbps'][df['trestbps'] > 100]\n",
    "mean_above_100 = values_above_100.mean()\n",
    "df['trestbps'] = df['trestbps'].apply(\n",
    "    lambda x: mean_above_100 if x < 100 or np.isnan(x) == True else x\n",
    ")\n",
    "values_between_0_and_4 = df['oldpeak'][(df['oldpeak'] >= 0) & (df['oldpeak'] <= 4)]\n",
    "mean_between_0_and_4 = values_between_0_and_4.mean()\n",
    "df['oldpeak'] = df['oldpeak'].apply(\n",
    "    lambda x: mean_between_0_and_4 if (x < 0 or x > 4) or np.isnan(x) == True else x\n",
    ")\n",
    "thaldur_mean = df['thaldur'].dropna().mean()\n",
    "thalach_mean = df['thalach'].dropna().mean()\n",
    "df['thaldur'].fillna(thaldur_mean, inplace=True)\n",
    "df['thalach'].fillna(thalach_mean, inplace=True)\n",
    "columns_to_replace = ['fbs', 'prop', 'nitr', 'pro', 'diuretic']\n",
    "df[columns_to_replace] = df[columns_to_replace].applymap(\n",
    "    lambda x: np.nan if x > 1 else x\n",
    ")\n",
    "for column in columns_to_replace:\n",
    "    mode_value = df[column].dropna().mode()[0]  # Get the mode\n",
    "    df[column].fillna(mode_value, inplace=True)\n",
    "exang_mode = df['exang'].dropna().mode()[0]\n",
    "slope_mode = df['slope'].dropna().mode()[0]\n",
    "df['exang'].fillna(exang_mode, inplace=True)\n",
    "df['slope'].fillna(slope_mode, inplace=True)\n",
    "df['source_1'] = df['smoke']\n",
    "df['source_2'] = df['smoke']\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source 1: Based on percentage, generate a random float beween 0-1 and if the value generated is less than the percentage in decimal form then impute missing value as 1, otherwise 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.abs.gov.au/statistics/health/health-conditions-and-risks/smoking-and-vaping/latest-release\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "part_of_caption = \"Proportion of people 15 years and over who were current daily smokers by age, 2011\"  # replace with your caption\n",
    "div = None\n",
    "for d in soup.find_all('div', {'class': 'chart-data-wrapper'}):\n",
    "    caption = d.find('pre', {'class': 'chart-caption'}).text\n",
    "    if part_of_caption in caption:\n",
    "        div = d\n",
    "        break\n",
    "# Extract chart data\n",
    "data = json.loads(div.find('pre', {'class': 'chart-data'}).text)\n",
    "smoking_2022 = np.array(data[7]).flatten() / 100\n",
    "dict = {(15,17) : smoking_2022[0], (18,24) : smoking_2022[1],(25,34) : smoking_2022[2], (35, 44) : smoking_2022[3], (45, 54) : smoking_2022[4], (55, 64) : smoking_2022[5], (65,74) : smoking_2022[6], (75,1000)  : smoking_2022[7]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, val in enumerate(df['source_1']):\n",
    "    if np.isnan(val) == True:\n",
    "        age = df.at[index, 'age']\n",
    "        for i in dict:\n",
    "            if age >= i[0] and age <= i[1]:\n",
    "                rand_val = np.random.rand()\n",
    "                df.iloc[index, df.columns.get_loc('source_1')] = 1.0 if rand_val <= dict[i] else 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source 2: Based on percentage, generate a random float beween 0-1 and if the value generated is less than the percentage in decimal form then impute missing value as 1, otherwise 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(18, 24): 0.053, (25, 44): 0.126, (45, 64): 0.149, (65, inf): 0.083}\n",
      "{(18, 24): 0.06874257425742575, (25, 44): 0.16342574257425743, (45, 64): 0.19325742574257426, (65, inf): 0.10765346534653468}\n"
     ]
    }
   ],
   "source": [
    "tobacco_data_url = \"https://www.cdc.gov/tobacco/data_statistics/fact_sheets/adult_data/cig_smoking/index.htm\"\n",
    "\n",
    "# Get the HTML content from the source\n",
    "response = requests.get(tobacco_data_url)\n",
    "if response.status_code != 200:\n",
    "    raise Exception(\"Failed to retrieve data.\")\n",
    "\n",
    "# Parse the HTML content with Scrapy's Selector\n",
    "html_content = response.content\n",
    "selector = Selector(text=html_content)\n",
    "\n",
    "# Extract relevant sections from the HTML content\n",
    "row_section = selector.xpath(\"//div[@class='row '][3]\")\n",
    "unordered_lists = row_section.xpath(\"//ul[@class='block-list']\")\n",
    "\n",
    "# Extract the text containing smoking rates by sex and age\n",
    "sex_rates = unordered_lists[0].xpath(\".//li/text()\").extract()  # Smoking rates by sex\n",
    "age_rates = unordered_lists[1].xpath(\".//li/text()\").extract()  # Smoking rates by age group\n",
    "\n",
    "# Get male and female smoking rates from extracted text\n",
    "male_smoking_rate = float(re.search(r\"\\((\\d+(\\.\\d+)?)%\", sex_rates[0]).group(1))\n",
    "female_smoking_rate = float(re.search(r\"\\((\\d+(\\.\\d+)?)%\", sex_rates[1]).group(1))\n",
    "\n",
    "# Create a dictionary to store age-based smoking rates\n",
    "age_based_rates = {}\n",
    "for rate_text in age_rates:\n",
    "    age_info = re.search(r\"aged (\\d+–\\d+|\\d+)\", rate_text).group(1)\n",
    "    smoking_percentage = float(re.search(r\"\\((\\d+(\\.\\d+)?)%\", rate_text).group(1))\n",
    "    \n",
    "    if \"–\" in age_info:\n",
    "        age_range = list(map(int, age_info.split(\"–\")))\n",
    "        age_based_rates[(age_range[0], age_range[1])] = smoking_percentage\n",
    "    else:\n",
    "        age_based_rates[(int(age_info), math.inf)] = smoking_percentage\n",
    "\n",
    "# Calculate adjusted rates for male patients based on the ratio between male and female smoking rates\n",
    "male_adjusted_rates = {\n",
    "    age_range: rate * (male_smoking_rate / female_smoking_rate)\n",
    "    for age_range, rate in age_based_rates.items()\n",
    "}\n",
    "\n",
    "female_rates = {key: value / 100 for key, value in age_based_rates.items()}\n",
    "male_rates = {key: value / 100 for key, value in male_adjusted_rates.items()}\n",
    "print(female_rates)\n",
    "print(male_rates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, val in enumerate(df['source_2']):\n",
    "    if np.isnan(val) == True:\n",
    "        age = df.at[index, 'age']\n",
    "        s =  df.at[index, 'sex']\n",
    "        if s == 1:\n",
    "            for i in male_rates:\n",
    "                if age >= i[0] and age <= i[1]:\n",
    "                    rand_val = np.random.rand()\n",
    "                    df.iloc[index, df.columns.get_loc('source_2')] = 1.0 if rand_val <= male_rates[i] else 0.0\n",
    "        else:\n",
    "            for i in female_rates:\n",
    "                if age >= i[0] and age <= i[1]:\n",
    "                    rand_val = np.random.rand()\n",
    "                    df.iloc[index, df.columns.get_loc('source_2')] = 1.0 if rand_val <= female_rates[i] else 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining source 1 and 2 columns to impute original smoke column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, val in enumerate(df['smoke']):\n",
    "    if np.isnan(val) == True:\n",
    "        s1 = df.at[index, 'source_1']\n",
    "        s2 = df.at[index, 'source_2']\n",
    "        df.iloc[index, df.columns.get_loc('smoke')] = 1.0 if s1 + s2 >= 1 else 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.10, stratify=y, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression CV Accuracy: 0.8059427957978682\n",
      "Decision Tree CV Accuracy: 0.714477417375968\n",
      "Random Forest CV Accuracy: 0.812161644045702\n"
     ]
    }
   ],
   "source": [
    "log_reg = LogisticRegression()\n",
    "log_reg_cv = cross_val_score(log_reg, X_train, y_train, cv=5, scoring='accuracy')\n",
    "\n",
    "print(\"Logistic Regression CV Accuracy:\", log_reg_cv.mean())\n",
    "\n",
    "decision_tree = DecisionTreeClassifier()\n",
    "decision_tree_cv = cross_val_score(decision_tree, X_train, y_train, cv=5, scoring='accuracy')\n",
    "\n",
    "print(\"Decision Tree CV Accuracy:\", decision_tree_cv.mean())\n",
    "\n",
    "random_forest = RandomForestClassifier()\n",
    "random_forest_cv = cross_val_score(random_forest, X_train, y_train, cv=5, scoring='accuracy')\n",
    "\n",
    "print(\"Random Forest CV Accuracy:\", random_forest_cv.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best according to the 5-fold cross validation is Logistic Regression\n",
      "Test set accuracy: 0.7444444444444445\n",
      "Classification report on test set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.70      0.71        40\n",
      "           1       0.76      0.78      0.77        50\n",
      "\n",
      "    accuracy                           0.74        90\n",
      "   macro avg       0.74      0.74      0.74        90\n",
      "weighted avg       0.74      0.74      0.74        90\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_model = None\n",
    "if log_reg_cv.mean() > max(decision_tree_cv.mean(), random_forest_cv.mean()):\n",
    "    best_model = log_reg\n",
    "elif decision_tree_cv.mean() > random_forest_cv.mean():\n",
    "    best_model = decision_tree\n",
    "else:\n",
    "    best_model = random_forest\n",
    "best_model.fit(X_train, y_train)\n",
    "y_pred = best_model.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"The best according to the 5-fold cross validation is Logistic Regression\")\n",
    "print(\"Test set accuracy:\", test_accuracy)\n",
    "\n",
    "print(\"Classification report on test set:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = '../staging_data/clean_table_HW2.csv'\n",
    "df.to_csv(csv_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "ac0dbcbce1613daf18e7f0841028aafb3272fae8b1c88851d3df7d48b5f98eaf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
